# -*- coding: utf-8 -*-
"""hackhathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kkr1O-iHXuO8KL1rw_WB8PEPxS7485qA

---
# **HACKHATHON**
#### Oleh  : Tim Anscorps
#### Email : athan.ipd6200@gmail.com
---

### Library dan Data
"""

# # installing third party hosted library
# !pip install pystan~=2.14
# !pip install fbprophet
# !pip install sklearn.mixture

!pip install factor_analyzer
!pip install umap_learn

# basic library
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('precision', 0)
np.seterr(divide='ignore', invalid='ignore')

# library code for LCA from https://github.com/dasirra/latent-class-analysis


class LCA:
    def __init__(self, n_components=2, tol=1e-3, max_iter=100, random_state=None):
        self.n_components = n_components
        self.random_state = random_state
        self.tol = tol
        self.max_iter = max_iter

        # flag to indicate if converged
        self.converged_ = False

        # model parameters
        self.ll_ = [-np.inf]
        self.weight = None
        self.theta = None
        self.responsibility = None

        # bic estimation
        self.bic = None

        # verbose level
        self.verbose = 0

    def _calculate_responsibility(self, data):

        n_rows, n_cols = np.shape(data)
        r_numerator = np.zeros(shape=(n_rows, self.n_components))
        for k in range(self.n_components):
            r_numerator[:, k] = self.weight[k] * np.prod(stats.bernoulli.pmf(
                data, p=self.theta[k]), axis=1)
        r_denominator = np.sum(r_numerator, axis=1)
        return r_numerator / np.tile(r_denominator, (self.n_components, 1)).T

    def _do_e_step(self, data):

        self.responsibility = self._calculate_responsibility(data)

    def _do_m_step(self, data):

        n_rows, n_cols = np.shape(data)

        # pi
        for k in range(self.n_components):
            self.weight[k] = np.sum(self.responsibility[:, k]) / float(n_rows)

        # theta
        for k in range(self.n_components):
            numerator = np.zeros((n_rows, n_cols))
            for n in range(n_rows):
                numerator[n, :] = self.responsibility[n, k] * data[n, :]
            numerator = np.sum(numerator, axis=0)
            denominator = np.sum(self.responsibility[:, k])
            self.theta[k] = numerator / denominator

        # correct numerical issues
        mask = self.theta > 1.0
        self.theta[mask] = 1.0
        mask = self.theta < 0.0
        self.theta[mask] = 0.0

    def fit(self, data):

        # initialization step
        n_rows, n_cols = np.shape(data)
        if n_rows < self.n_components:
            raise ValueError(
                '''
                LCA estimation with {n_components} components, but got only
                {n_rows} samples
                '''.format(n_components=self.n_components, n_rows=n_rows))

        if self.verbose > 0:
            print('EM algorithm started')

        self.weight = stats.dirichlet.rvs(np.ones(shape=self.n_components) / 2, random_state=self.random_state)[0]
        self.theta = stats.dirichlet.rvs(alpha=np.ones(shape=n_cols) / 2,
                                         size=self.n_components,
                                         random_state=self.random_state)

        for i in range(self.max_iter):
            if self.verbose > 0:
                print('\tEM iteration {n_iter}'.format(n_iter=i))

            # E-step
            self._do_e_step(data)

            # M-step
            self._do_m_step(data)

            # Check for convergence
            aux = np.zeros(shape=(n_rows, self.n_components))
            for k in range(self.n_components):
                normal_prob = np.prod(stats.bernoulli.pmf(data, p=self.theta[k]), axis=1)
                aux[:, k] = self.weight[k] * normal_prob
            ll_val = np.sum(np.log(np.sum(aux, axis=1)))
            if np.abs(ll_val - self.ll_[-1]) < self.tol:
                break
            else:
                self.ll_.append(ll_val)

        # calculate bic
        self.bic = np.log(n_rows)*(sum(self.theta.shape)+len(self.weight)) - 2.0*self.ll_[-1]

    def predict(self, data):
        return np.argmax(self.predict_proba(data), axis=1)

    def predict_proba(self, data):
        return self._calculate_responsibility(data)

"""### Persiapan APBD dan Pemecahan Berdasarkan Kode Akun Utama"""

# data apbd
raw_data_apbd = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/A2022_dataset_buat bedah data.csv", delimiter=";")

raw_data_apbd.tail(10)

print(raw_data_apbd.apply(lambda col: col.unique()))

jumlah_data_apbd_awal = len(raw_data_apbd)
print("Jumlah baris data awal : ", jumlah_data_apbd_awal)

# select column yang mau diambil
raw_data_apbd = raw_data_apbd[["kodepemda","namapemda","kodefungsi","namafungsi","kodeakunutama", "kodeakunkelompok", "kodeakunjenis", "kodeakunobjek", "kodeakunrinci", "kodeakunsubrinci", "namaakunsubrinci", "nilaianggaran", "namasumberdana"]]

raw_data_apbd.head(10)

print(raw_data_apbd['kodefungsi'].unique())
print(len(raw_data_apbd['kodefungsi'].unique()))

# pembersihan nama fungsi
raw_data_apbd['namafungsi'].fillna("-")
raw_data_apbd['namafungsi'] = raw_data_apbd['namafungsi'].str.partition("|")[0]
raw_data_apbd['namafungsi'] = raw_data_apbd['namafungsi'].str.lower().str.strip().str.replace('&amp;', 'dan')

print(raw_data_apbd['namafungsi'].unique())
print(len(raw_data_apbd['namafungsi'].unique()))

# change kode to numeric
raw_data_apbd['kodeakunutama'] = pd.to_numeric(raw_data_apbd["kodeakunutama"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['kodeakunkelompok'] = pd.to_numeric(raw_data_apbd["kodeakunkelompok"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['kodeakunjenis'] = pd.to_numeric(raw_data_apbd["kodeakunjenis"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['kodeakunobjek'] = pd.to_numeric(raw_data_apbd["kodeakunobjek"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['kodeakunrinci'] = pd.to_numeric(raw_data_apbd["kodeakunrinci"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['kodeakunsubrinci'] = pd.to_numeric(raw_data_apbd["kodeakunsubrinci"], errors='coerce', downcast='integer').round().astype('Int64', errors='ignore')
raw_data_apbd['rekening_neraca'] = raw_data_apbd['kodeakunutama'].astype('Int64', errors='ignore').map(str) +'.'+ raw_data_apbd['kodeakunkelompok'].astype('Int64', errors='ignore').map(str) +'.' + raw_data_apbd['kodeakunjenis'].astype('Int64', errors='ignore').map(str) +'.' + raw_data_apbd['kodeakunobjek'].astype('Int64', errors='ignore').map(str) +'.' + raw_data_apbd['kodeakunrinci'].astype('Int64', errors='ignore').map(str) +'.' + raw_data_apbd['kodeakunsubrinci'].astype('Int64', errors='ignore').map(str)

raw_data_apbd['kodeakunutama'].unique()

# pisah dataset
apbd_transfer = raw_data_apbd[raw_data_apbd["kodeakunutama"] == 6]
apbd_transfer.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_transfer.csv") 
apbd_pendapatan = raw_data_apbd[raw_data_apbd["kodeakunutama"] == 4]
apbd_pendapatan.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_pendapatan.csv") 
apbd_belanja = raw_data_apbd[raw_data_apbd["kodeakunutama"] == 5]
apbd_belanja.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_belanja.csv")

"""### Pengolahan Awal APBD, Umur Harapan Hidup, Indeks Pembangunan Manusia, dan Harapan Lama Sekolah"""

# load data APBD BELANJA
apbd_belanja = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_belanja.csv")

# olah untuk belanja
# drop row yang null
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunutama'].notnull()]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunkelompok'].notnull()]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunjenis'].notnull()]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunobjek'].notnull()]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunrinci'].notnull()]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunsubrinci'].notnull()]

# drop row dengan kode nol
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunutama'] > 0]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunkelompok'] > 0]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunjenis'] > 0]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunobjek'] > 0]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunrinci'] > 0]
apbd_belanja = apbd_belanja[apbd_belanja['kodeakunsubrinci'] > 0]

apbd_belanja.tail(5)

# membuat data total anggarang berdasarkan regional, fungsi, dan rekening neraca dengan minimal nil
apbd_belanja = apbd_belanja.groupby(['kodepemda','namapemda', 'kodefungsi', 'namafungsi','rekening_neraca', 'namaakunsubrinci'])['nilaianggaran'].sum().reset_index()

apbd_belanja.head(10)

# menghitung jumlah rekening neraca pada data
print(len(apbd_belanja['rekening_neraca'].unique()))

# membuat pivot tabel
apbd_belanja_ringkasan = pd.pivot_table(apbd_belanja, fill_value=0, columns=['rekening_neraca', 'namafungsi'], values=['nilaianggaran'], index=['kodepemda', 'namapemda'], aggfunc={'nilaianggaran': np.sum}).reset_index()

apbd_belanja_ringkasan.head(10)

# menyeragamkan nama regional
apbd_belanja_ringkasan['namapemda'] = apbd_belanja_ringkasan['namapemda'].str.lower().str.replace('kab.', '').str.replace('kep.', 'kepulauan').str.strip()

# merubah pivot tabel multidimensi menjadi dataframe
apbd_belanja_ringkasan = pd.DataFrame(apbd_belanja_ringkasan.to_records())
apbd_belanja_ringkasan.columns = [str(kolom.replace("(", "").replace(")", "").replace(",", "_").replace("'","").replace(" ", "")) for kolom in apbd_belanja_ringkasan.columns]
apbd_belanja_ringkasan.reset_index()

# merubah nama kolom
apbd_belanja_ringkasan.rename(columns = {'kodepemda__':'kodepemda', 'namapemda__' : 'namapemda'}, inplace = True)

apbd_belanja_ringkasan.head(5)

apbd_belanja_ringkasan.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_belanja_ringkasan.csv")

apbd_belanja_ringkasan['namapemda']

apbd_belanja_ringkasan = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_belanja_ringkasan.csv")

# load data APBD BELANJA
uhh = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/bps_uhh2021.csv")
hls = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/bps_hls2021.csv")
ipm = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/bps_ipm2021.csv")
outkapita = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/bps_outkapita2021.csv")

# ubah nama kolom
uhh.columns = ['wilayah_uhh', '2021_uhh']
ipm.columns = ['wilayah_ipm', '2021_ipm']
hls.columns = ['wilayah_hls', '2021_hls']
outkapita.columns = ['wilayah_outkapita', '2021_outkapita']

# menyeragamkan nama wilayah
uhh['wilayah_uhh'] = uhh['wilayah_uhh'].str.lower().str.strip()
ipm['wilayah_ipm'] = ipm['wilayah_ipm'].str.lower().str.strip()
hls['wilayah_hls'] = hls['wilayah_hls'].str.lower().str.strip()
outkapita['wilayah_outkapita'] = outkapita['wilayah_outkapita'].str.lower().str.strip()

apbd_belanja_ringkasan = pd.merge(apbd_belanja_ringkasan, uhh, left_on='namapemda', right_on='wilayah_uhh', how='left', suffixes="_uhh")
apbd_belanja_ringkasan = pd.merge(apbd_belanja_ringkasan, hls, left_on='namapemda', right_on='wilayah_hls', how='left', suffixes="_hls")
apbd_belanja_ringkasan = pd.merge(apbd_belanja_ringkasan, ipm, left_on='namapemda', right_on='wilayah_ipm', how='left', suffixes="_ipm")
apbd_belanja_ringkasan = pd.merge(apbd_belanja_ringkasan, outkapita, left_on='namapemda', right_on='wilayah_outkapita', how='left', suffixes="_outkapita")

apbd_belanja_ringkasan.head(10)

del apbd_belanja_ringkasan['wilayah_hls']
del apbd_belanja_ringkasan['wilayah_uhh']
del apbd_belanja_ringkasan['wilayah_ipm']
del apbd_belanja_ringkasan['wilayah_outkapita']
del apbd_belanja_ringkasan['index']

apbd_belanja_ringkasan.dropna(inplace=True)

# simpan sementara
apbd_belanja_ringkasan.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_ipm_hls_uhh.csv", index=False)

"""### Preprocessing Data Gabungan"""

# read data
apbd_ipm_hls_uhh = pd.read_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/apbd_ipm_hls_uhh.csv").reset_index()

apbd_ipm_hls_uhh.dropna(inplace=True)

apbd_ipm_hls_uhh.head(10)

apbd_ipm_hls_uhh.shape

indikator_data = apbd_ipm_hls_uhh.iloc[:, -4:]

indikator_data.head(10)

print(indikator_data[indikator_data['2021_hls'] == None])
print(indikator_data[indikator_data['2021_uhh'] == None])
print(indikator_data[indikator_data['2021_ipm'] == None])
print(indikator_data[indikator_data['2021_outkapita'] == None])

indikator_data.values

indikator_data.isnull().any()

# memnetukan jumlah cluster untuk dependen
n_components = np.arange(1, 10)
models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(indikator_data.values.astype(np.float)) for n in n_components]

plt.plot(n_components, [m.bic(indikator_data.values.astype(np.float)) for m in models], label='BIC')
plt.plot(n_components, [m.aic(indikator_data.values.astype(np.float)) for m in models], label='AIC')
plt.legend(loc='best')
plt.xlabel('n_components');

# dari hasil analisa diatas, didapatkan jumlah sluster optimal 3
label3 = models[2].predict(indikator_data.values.astype(np.float))
# tambahan untuk label 2
label2 = models[1].predict(indikator_data.values.astype(np.float))

# distribusi frekuensi dari masing - masing kelas
unique, counts = np.unique(label3, return_counts=True)
print(dict(zip(unique, counts)))
# distribusi frekuensi dari masing - masing kelas
unique, counts = np.unique(label2, return_counts=True)
print(dict(zip(unique, counts)))

# embed label cluster ke data awal
apbd_ipm_hls_uhh['cl3'] = label3
apbd_ipm_hls_uhh['cl2'] = label2

# check wilayah dengan kategori paling sedikit
print("CLUSTER 3 JENIS")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 0].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 1].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 2 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 2].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("---------------------------------------------------------------------------------------------------------------------------------")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 0].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 1].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 2 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 2].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())

# check wilayah dengan kategori paling sedikit
print("CLUSTER 2 JENIS")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 0].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 1].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("---------------------------------------------------------------------------------------------------------------------------------")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 0].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 1].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())

# plot data
plt.scatter(apbd_ipm_hls_uhh['2021_ipm'], apbd_ipm_hls_uhh['2021_outkapita'], c=apbd_ipm_hls_uhh['cl3'], alpha = 0.7)

# title and labels
plt.title('Cluster Tiga Wilayah\n', loc='left', fontsize=22)
plt.xlabel('Indek Pembangunan Manusia')
plt.ylabel('Pengeluaran Per Kapita(Ribu Rupiah)')

# plot data
plt.scatter(apbd_ipm_hls_uhh['2021_ipm'], apbd_ipm_hls_uhh['2021_outkapita'], c=apbd_ipm_hls_uhh['cl2'], alpha = 0.7)

# title and labels
plt.title('Wilayah Dua Cluster\n', loc='left', fontsize=22)
plt.xlabel('Indek Pembangunan Manusia')
plt.ylabel('Pengeluaran Per Kapita(Ribu Rupiah)')

"""### Dimension Reduction untuk Rekening Neraca"""

# Scaling data
# define data for scaled
rekening_data = apbd_ipm_hls_uhh.iloc[:, 4:-6]
print(rekening_data.columns)

rekening_data.shape

rekening_data.describe()

total_temp = []
for column in rekening_data.columns:
  total_temp.append([column, rekening_data[column].sum()])

total_temp = pd.DataFrame(total_temp)
total_temp.columns = ['rekening_fungsi', 'nilai']
total_temp.sort_values(by=['nilai'], ascending=False).head(10)

total_temp.describe()

# define standard scaler
scaler = StandardScaler()
# transform data
rekening_data_scaled = scaler.fit_transform(rekening_data)
print(rekening_data_scaled)



for i in np.linspace(0.7, 0.99, num=30):
  pca = PCA(n_components = i)
  pca.fit(rekening_data_scaled)
  reduced = pca.transform(rekening_data_scaled)
  print("Variance cumulative : ", i, " Num of components : ", pca.n_components_)

# !pip install factor_analyzer
# !pip install umap_learn

# pengujian dengan faktor analysis
# from factor_analyzer import FactorAnalyze
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(rekening_data_scaled)
chi_square_value, p_value

"""Tidak bisa menggunakan Factor Analysis karena prasyarat tidak terpenuhi (p-value < 0.05)"""

from sklearn.decomposition import TruncatedSVD
cumulative_variance = 0
i = 5
while cumulative_variance < 0.95:
  svd = TruncatedSVD(n_components=i, random_state=42).fit_transform(rekening_data_scaled)
  # print(svd)
  cumulative_variance = sum(np.var(svd, axis=0) / np.var(rekening_data_scaled, axis=0).sum())
  print("Variance cumulative : ", cumulative_variance, " Num of components : ", i)
  i += 5

# data menggunakan reduksi berdasarkan linear, menggunakan n_components = 70 
# karena variansi yang di cover dapat dikatakn baik (Rules of thumb)
# reduksi dari PCA
pca = PCA(n_components = 70)
pca.fit(rekening_data_scaled)
x_pca = pca.transform(rekening_data_scaled)

pca_full = PCA(n_components = 320)
pca_full.fit(rekening_data_scaled)
x_pca_full = pca_full.transform(rekening_data_scaled)

# reduksi dari SVD
x_svd = TruncatedSVD(n_components=70, random_state=42).fit_transform(rekening_data_scaled)
x_svd_full = TruncatedSVD(n_components=320, random_state=42).fit_transform(rekening_data_scaled)

# reduksi dimensi pendekatan non linear
# umap
import umap
x_umap = umap.UMAP(
    n_components=70,
    n_neighbors=5,
    min_dist=0.3,
    metric='correlation'
).fit_transform(rekening_data_scaled)

x_umap_full = umap.UMAP(
    n_components=320,
    n_neighbors=5,
    min_dist=0.3,
    metric='correlation'
).fit_transform(rekening_data_scaled)

# isomap
from sklearn.manifold import Isomap
x_isomap = Isomap(n_components=70).fit_transform(rekening_data_scaled)
x_isomap_full = Isomap(n_components=320).fit_transform(rekening_data_scaled)

print(x_umap.shape)
print(x_pca.shape)
print(x_svd.shape)
print(x_isomap.shape)
print("----------------")
print(x_umap_full.shape)
print(x_pca_full.shape)
print(x_svd_full.shape)
print(x_isomap_full.shape)
print(rekening_data.shape)

# pembuatan dataset
dataset_pca_simple = pd.merge(pd.DataFrame(x_pca), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_svd_simple = pd.merge(pd.DataFrame(x_svd), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_isomap_simple = pd.merge(pd.DataFrame(x_isomap), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_umap_simple = pd.merge(pd.DataFrame(x_umap), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)

dataset_pca_full = pd.merge(pd.DataFrame(x_pca_full), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_svd_full = pd.merge(pd.DataFrame(x_svd_full), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_isomap_full = pd.merge(pd.DataFrame(x_isomap_full), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)
dataset_umap_full = pd.merge(pd.DataFrame(x_umap_full), apbd_ipm_hls_uhh.iloc[:,-2:], left_index=True, right_index=True)

# Buat dictionary untuk list dataset
datasets = {
    'PCA Simple': dataset_pca_simple, 
    'SVD Simple': dataset_svd_simple, 
    'Isomap Simple': dataset_isomap_simple,
    'UMAP Simple' : dataset_umap_simple,
    'PCA Full': dataset_pca_full, 
    'SVD Full': dataset_svd_full, 
    'Isomap Full': dataset_isomap_full,
    'UMAP Full' : dataset_umap_full,
    }

apbd_ipm_hls_uhh.to_csv("/content/drive/MyDrive/20221016 Hackhathon Kemenkeu 2022/Dataset/data_labeled.csv")

"""### Modelling"""

!pip install pycm

from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_curve
from sklearn.tree import export_graphviz
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
import tensorflow as tf
from tensorflow.keras import backend
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from pycm import ConfusionMatrix
import random
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# persiapan pemodelan
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

backend.clear_session()
np.random.seed(42)
random.seed(42)
models = {}
mmodels = {}

# Logistic Regression
from sklearn.linear_model import LogisticRegression
models['Logistic Regression'] = LogisticRegression(solver='liblinear', random_state=42, verbose=1, dual=False)

# Support Vector Machines
from sklearn.svm import LinearSVC
models['Support Vector Machines'] = LinearSVC(verbose=1)
mmodels['Support Vector Machines'] = LinearSVC(verbose=1)

# Decision Trees 
from sklearn.tree import DecisionTreeClassifier
models['Decision Trees'] = DecisionTreeClassifier()
mmodels['Decision Trees'] = DecisionTreeClassifier()

# Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
models['Random Forest Klasifikasi'] = RandomForestClassifier(verbose=1, random_state = 42)
mmodels['Random Forest Klasifikasi'] = RandomForestClassifier(verbose=1, random_state = 42)

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
models['Random Forest Regresi'] = RandomForestRegressor(random_state = 1, verbose=42)

# Naive Bayes
from sklearn.naive_bayes import GaussianNB
models['Naive Bayes'] = GaussianNB()
mmodels['Naive Bayes'] = GaussianNB()

# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier
models['K-Nearest Neighbor'] = KNeighborsClassifier()
mmodels['K-Nearest Neighbor'] = KNeighborsClassifier()

# AdaBoost
from sklearn.ensemble import AdaBoostClassifier
models['AdaBoost'] = AdaBoostClassifier()
mmodels['AdaBoost'] = AdaBoostClassifier()

# XGBBoost
from xgboost import XGBClassifier
models['XGBBoost'] = XGBClassifier()
mmodels['XGBBoost'] = XGBClassifier()

plt.rc("font", size=14)
sns.set(style="white")
sns.set(style="whitegrid", color_codes=True)

# Distribusi Cluster untuk Dua Cluster
# distribusi frekuensi dari masing - masing kelas
unique, counts = np.unique(label2, return_counts=True)
print("Distribusi Dua Cluster",dict(zip(unique, counts)),"\n")

# check wilayah dengan kategori paling sedikit
print("CLUSTER 2 JENIS")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 0].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 1].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("---------------------------------------------------------------------------------------------------------------------------------")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 0].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl2'] == 1].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())

# looping model untuk masing - masing data
print("Untuk Dua Cluster")
accuracy, precision, recall, predictions_tr, predictions_train_tr, train_predicts, train_actuals, test_predicts, test_actuals, score_tr = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}
for dataset_key in datasets.keys():
  for key in models.keys():
      print("Nama dataset : ", dataset_key)
      print("Nama model : ", key)
      name = "Dua Cluster " + key + " " + dataset_key + " tanpa Imbalance Treatement"

      # tanpa transformasi
      X, y = datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-1:]
      unique, counts = np.unique(y, return_counts=True)
      print("Distribusi Dua Cluster",dict(zip(unique, counts)),"\n")

      X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

      # Fit the classifier model
      with warnings.catch_warnings():
          # ignore all caught warnings
          warnings.filterwarnings("ignore")
          # execute code that will generate warnings
          models[key].fit(X_train_temp_tr, y_train_temp_tr)
      
      # Prediction 
      prediction_tr = models[key].predict(X_test_temp_tr)
      prediction_train_tr = models[key].predict(X_train_temp_tr)
      
      if len(set(prediction_tr)) != 2:
          pre, rec, thre = precision_recall_curve(y_test_temp_tr, prediction_tr)
          print('Thresold', thre[np.argmax(pre*rec)])
          for i in range(0, len(prediction_tr)):
              if(prediction_tr[i] >= thre[np.argmax(pre*rec)]):
                  prediction_tr[i] = 1
              else:
                  prediction_tr[i] = 0
      
      # Calculate Accuracy, Precision and Recall Metrics
      accuracy[name] = accuracy_score(prediction_tr, y_test_temp_tr)
      precision[name] = precision_score(prediction_tr, y_test_temp_tr, average=None)
      recall[name] = recall_score(prediction_tr, y_test_temp_tr, average=None)
      score_tr[name] = models[key].score(X_train_temp_tr, y_train_temp_tr)

      train_predicts[name] = np.array(prediction_train_tr)
      train_actuals[name] = np.array(y_train_temp_tr)
      test_predicts[name] = np.array(prediction_tr)
      test_actuals[name] = np.array(y_test_temp_tr)
      
      # cm[name] = cm
      predictions_tr[name] = prediction_tr
      predictions_train_tr[name] = prediction_train_tr
      print('************************************\n\n')

for dataset_key in datasets.keys():
  for key in models.keys():
      print("Nama dataset : ", dataset_key)
      print("Nama model : ", key)
      name = "Dua Cluster " + key + " " + dataset_key + " dengan Imbalance Treatement"

      # define pipeline
      over = SMOTE(sampling_strategy=0.3)
      steps = [('o', over)]
      pipeline = Pipeline(steps=steps)
      # transform the dataset
      X, y = pipeline.fit_resample(datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-1:])
      unique, counts = np.unique(y, return_counts=True)
      print("Distribusi Dua Cluster",dict(zip(unique, counts)),"\n")

      X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

      # Fit the classifier model
      with warnings.catch_warnings():
          # ignore all caught warnings
          warnings.filterwarnings("ignore")
          # execute code that will generate warnings
          models[key].fit(X_train_temp_tr, y_train_temp_tr)
      
      # Prediction 
      prediction_tr = models[key].predict(X_test_temp_tr)
      prediction_train_tr = models[key].predict(X_train_temp_tr)
      
      if len(set(prediction_tr)) != 2:
          pre, rec, thre = precision_recall_curve(y_test_temp_tr, prediction_tr)
          print('Thresold', thre[np.argmax(pre*rec)])
          for i in range(0, len(prediction_tr)):
              if(prediction_tr[i] >= thre[np.argmax(pre*rec)]):
                  prediction_tr[i] = 1
              else:
                  prediction_tr[i] = 0
      
      # Calculate Accuracy, Precision and Recall Metrics
      accuracy[name] = accuracy_score(prediction_tr, y_test_temp_tr)
      precision[name] = precision_score(prediction_tr, y_test_temp_tr, average=None)
      recall[name] = recall_score(prediction_tr, y_test_temp_tr, average=None)
      score_tr[name] = models[key].score(X_train_temp_tr, y_train_temp_tr)

      train_predicts[name] = np.array(prediction_train_tr)
      train_actuals[name] = np.array(y_train_temp_tr)
      test_predicts[name] = np.array(prediction_tr)
      test_actuals[name] = np.array(y_test_temp_tr)
      
      # cm[name] = cm
      predictions_tr[name] = prediction_tr
      predictions_train_tr[name] = prediction_train_tr
      print('************************************\n\n')

for dataset_key in datasets.keys():
    print("Nama dataset : ", dataset_key)
    print("Nama model : ", "Deep Learning")
    name = "Dua Cluster " + "Deep Learning" + " " + dataset_key + " tanpa Imbalance Treatement"

    # tanpa transformasi
    X, y = datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-1:]
    unique, counts = np.unique(y, return_counts=True)
    print("Distribusi Dua Cluster",dict(zip(unique, counts)),"\n")

    X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

    # Fit the classifier model
    with warnings.catch_warnings():
        # ignore all caught warnings
        warnings.filterwarnings("ignore")
        # execute code that will generate warnings
        backend.clear_session()
        np.random.seed(42)
        random.seed(42)

        dl_dua = Sequential()
        dl_dua.add(Dense(32, activation='relu', input_dim=X_train_temp_tr.shape[1]))
        dl_dua.add(Dense(1, activation='sigmoid'))
        optimizer = tf.keras.optimizers.Adam(0.001)
        dl_dua.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'] )
        
    history = dl_dua.fit(X_train_temp_tr, y_train_temp_tr, validation_split=0.2, epochs=100, batch_size=32, verbose=1)
    
    # Prediction 
    prediction_tr = dl_dua.predict(X_test_temp_tr)
    prediction_train_tr = dl_dua.predict(X_train_temp_tr)
    
    pre, rec, thre = precision_recall_curve(y_test_temp_tr, prediction_tr)
    print('Thresold', thre[np.argmax(pre*rec)])
    for i in range(0, len(prediction_tr)):
        if(prediction_tr[i] >= thre[np.argmax(pre*rec)]):
            prediction_tr[i] = 1
        else:
            prediction_tr[i] = 0
    
    pre, rec, thre = precision_recall_curve(y_train_temp_tr, prediction_train_tr)
    print('Thresold', thre[np.argmax(pre*rec)])
    for i in range(0, len(prediction_train_tr)):
        if(prediction_train_tr[i] >= thre[np.argmax(pre*rec)]):
            prediction_train_tr[i] = 1
        else:
            prediction_train_tr[i] = 0

    # Calculate Accuracy, Precision and Recall Metrics
    accuracy[name] = accuracy_score(prediction_tr, y_test_temp_tr)
    precision[name] = precision_score(prediction_tr, y_test_temp_tr, average=None)
    recall[name] = recall_score(prediction_tr, y_test_temp_tr, average=None)
    score_tr[name] = roc_auc_score(prediction_train_tr, y_train_temp_tr)

    train_predicts[name] = np.array(prediction_train_tr)
    train_actuals[name] = np.array(y_train_temp_tr)
    test_predicts[name] = np.array(prediction_tr)
    test_actuals[name] = np.array(y_test_temp_tr)
    
    # cm[name] = cm
    predictions_tr[name] = prediction_tr
    predictions_train_tr[name] = prediction_train_tr
    print('************************************\n\n')

for dataset_key in datasets.keys():
    print("Nama dataset : ", dataset_key)
    print("Nama model : ", "Deep Learning")
    name = "Dua Cluster " + "Deep Learning" + " " + dataset_key + " dengan Imbalance Treatement"

    # define pipeline
    over = SMOTE(sampling_strategy=0.3)
    steps = [('o', over)]
    pipeline = Pipeline(steps=steps)
    # transform the dataset
    X, y = pipeline.fit_resample(datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-1:])
    unique, counts = np.unique(y, return_counts=True)
    print("Distribusi Dua Cluster",dict(zip(unique, counts)),"\n")

    X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

    # Fit the classifier model
    with warnings.catch_warnings():
        # ignore all caught warnings
        warnings.filterwarnings("ignore")
        # execute code that will generate warnings
        backend.clear_session()
        np.random.seed(42)
        random.seed(42)

        dl_dua = Sequential()
        dl_dua.add(Dense(32, activation='relu', input_dim=X_train_temp_tr.shape[1]))
        dl_dua.add(Dense(1, activation='sigmoid'))
        optimizer = tf.keras.optimizers.Adam(0.001)
        dl_dua.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'] )
        
    history = dl_dua.fit(X_train_temp_tr, y_train_temp_tr, validation_split=0.2, epochs=100, batch_size=32, verbose=1)
      
    # Prediction 
    prediction_tr = dl_dua.predict(X_test_temp_tr)
    prediction_train_tr = dl_dua.predict(X_train_temp_tr)
    
    pre, rec, thre = precision_recall_curve(y_test_temp_tr, prediction_tr)
    print('Thresold', thre[np.argmax(pre*rec)])
    for i in range(0, len(prediction_tr)):
        if(prediction_tr[i] >= thre[np.argmax(pre*rec)]):
            prediction_tr[i] = 1
        else:
            prediction_tr[i] = 0
    
    pre, rec, thre = precision_recall_curve(y_train_temp_tr, prediction_train_tr)
    print('Thresold', thre[np.argmax(pre*rec)])
    for i in range(0, len(prediction_train_tr)):
        if(prediction_train_tr[i] >= thre[np.argmax(pre*rec)]):
            prediction_train_tr[i] = 1
        else:
            prediction_train_tr[i] = 0


    
    # Calculate Accuracy, Precision and Recall Metrics
    accuracy[name] = accuracy_score(prediction_tr, y_test_temp_tr)
    precision[name] = precision_score(prediction_tr, y_test_temp_tr, average=None)
    recall[name] = recall_score(prediction_tr, y_test_temp_tr, average=None)
    score_tr[name] = score_tr[name] = roc_auc_score(prediction_train_tr, y_train_temp_tr)

    train_predicts[name] = np.array(prediction_train_tr)
    train_actuals[name] = np.array(y_train_temp_tr)
    test_predicts[name] = np.array(prediction_tr)
    test_actuals[name] = np.array(y_test_temp_tr)
    
    # cm[name] = cm
    predictions_tr[name] = prediction_tr
    predictions_train_tr[name] = prediction_train_tr
    print('************************************\n\n')

cm_tr, df_new_tr = {}, {}
metode_arr = []
better_models = []

for key in score_tr.keys():
    print(key)
    
    train_predict_tr = train_predicts[key]
    train_actual_tr = train_actuals[key]
    test_predict_tr = test_predicts[key]
    test_actual_tr = test_actuals[key]
    
    matrix_tr = classification_report(test_actual_tr, test_predict_tr)
    accuracy_temp = accuracy_score(test_predict_tr, test_actual_tr)
    precision_temp = precision_score(test_predict_tr, test_actual_tr, average=None)
    recall_temp = recall_score(test_predict_tr, test_actual_tr, average=None)

    print('Accuracy : ', accuracy_temp)
    print('Presisi : ', precision_temp)
    print('Recall : ', recall_temp)
    print('Classification report : \n',matrix_tr)

    metode_arr.append({'Nama Pemodelan' : key, 'Akurasi' : accuracy_temp, 'Presisi 0' : precision_temp[0], 'Presisi 1' : precision_temp[1], 'Recall 0' : recall_temp[0], 'Recall 1' : recall_temp[1]})
    
    #grafik untuk precision recall
    #calculate precision and recall
    precision_temp_tr, recall_temp_tr, thresholds_temp_tr = precision_recall_curve(test_actual_tr, test_predict_tr)


    #create precision recall curve
    fig, ax = plt.subplots()
    plt.plot(recall_temp_tr, precision_temp_tr, color='purple')

    #add axis labels to plot
    ax.set_title('Precision-Recall Curve')
    ax.set_ylabel('Precision')
    ax.set_xlabel('Recall')

    #display plot
    plt.show()
    
    #grafik untuk auc-roc
    ns_fpr, ns_tpr, _ = roc_curve(train_actual_tr, train_predict_tr)
    # plot the roc curve for the model
    plt.plot(ns_fpr, ns_tpr, linestyle='--', label=key)
    # axis labels
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    # show the legend
    plt.legend()
    # show the plot
    plt.show()
    print('\n\n')

metode_df = pd.DataFrame.from_dict(metode_arr)
metode_df

# Data tanpa penangan Imbalance
metode_df[metode_df['Nama Pemodelan'].str.contains("tanpa")].sort_values(by=['Akurasi', 'Presisi 0', 'Presisi 1', 'Recall 0', 'Recall 1'], ascending=False)

# Data tanpa penangan Imbalance
metode_df[metode_df['Nama Pemodelan'].str.contains("dengan")].sort_values(by=['Akurasi', 'Presisi 0', 'Presisi 1', 'Recall 0', 'Recall 1'], ascending=False)

"""Dari beberapa hasil diatas, didapatkan beberapa kesimpulan terkait pemodelan dengan **dua kluster** antara lain:

*   Pemodelan dengan akurasi, presisi, dan recall yang paling baik dalam model yang diajukan sebagian besar menggunakan metode Random Forest (Klasifikasi dan Regresi dengan *thresold*), selain itu metode XGBBoost juga menunjukkan pembentukan model yang baik pada dua kluster.
*   Data penanganan *imbalance data* menggunakan oversampling dengan menjadikan jumlah label minor sebesar 30% dari label mayor mampu meningkatkan akurasi, presisi, dan recall masing - masing kluster diatas 90%
*   Hasil reduksi fitur dengan mempertahankan *cumulative variance* sebesar 70% (70 kolom baru, simple) menunjukkan hasil yang lebih baik dibandingkan *cumulative variance* sebesar 95% (312 kolom baru, full). Reduksi fitur/dimensi digunakan karena pemodelan rekap data yang jomplang (kolom 16k, row hanya 500an)
* Reduksi yang dilakukan dengan menggunakan metode linier seperti PCA dan SVD menunjukkan pembentukan model yang baik dengan beberapa metode klasifiaksi. Hal ini ditunjukkan dengan dominasi akurasi tertinggi dengan menggunakan reduksi dimensi linier. 
* Untuk dua cluster, penanganan reduksi dimensi dengan *cumulative variance* sebesar 70% (dimensi 70) juga menunjukkan pemodelan yang lebih baik daripada pemodelan engan *cumulative variance* 95% (dimensi 320)

"""

# Distribusi Cluster untuk Tiga Cluster
# distribusi frekuensi dari masing - masing kelas
unique, counts = np.unique(label3, return_counts=True)
print("Distribusi Tiga Cluster",dict(zip(unique, counts)),"\n")

# check wilayah dengan kategori paling sedikit
print("CLUSTER 3 JENIS")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 0].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 1].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("Cluster 2 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 2].sort_values(by=['2021_ipm'], ascending=True).head(10)['namapemda'].to_list())
print("---------------------------------------------------------------------------------------------------------------------------------")
print("Cluster 0 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 0].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 1 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 1].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())
print("Cluster 2 : ",apbd_ipm_hls_uhh[apbd_ipm_hls_uhh['cl3'] == 2].sort_values(by=['2021_ipm'], ascending=False).head(10)['namapemda'].to_list())

# looping model untuk masing - masing data
print("Untuk Tiga Cluster")
accuracy3, precision3, recall3, predictions_tr3, predictions_train_tr3, train_predicts3, train_actuals3, test_predicts3, test_actuals3, score_tr3 = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}
for dataset_key in datasets.keys():
  for key in mmodels.keys():
      print("Nama dataset : ", dataset_key)
      print("Nama model : ", key)
      name = "Tiga Cluster " + key + " " + dataset_key + " tanpa Imbalance Treatement"

      # tanpa transformasi
      X, y = datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-2:-1]
      unique, counts = np.unique(y, return_counts=True)
      print("Distribusi Tiga Cluster",dict(zip(unique, counts)),"\n")

      X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.20, random_state=42)

      # Fit the classifier model
      with warnings.catch_warnings():
          # ignore all caught warnings
          warnings.filterwarnings("ignore")
          # execute code that will generate warnings
          mmodels[key].fit(X_train_temp_tr, y_train_temp_tr)
      
      # Prediction 
      prediction_tr3 = mmodels[key].predict(X_test_temp_tr)
      prediction_train_tr3 = mmodels[key].predict(X_train_temp_tr)
      
      
      # Calculate Accuracy, Precision and Recall Metrics
      accuracy3[name] = accuracy_score(prediction_tr3, y_test_temp_tr)
      precision3[name] = precision_score(prediction_tr3, y_test_temp_tr, average=None)
      recall3[name] = recall_score(prediction_tr3, y_test_temp_tr, average=None)
      score_tr3[name] = mmodels[key].score(X_train_temp_tr, y_train_temp_tr)

      train_predicts3[name] = np.array(prediction_train_tr3)
      train_actuals3[name] = np.array(y_train_temp_tr)
      test_predicts3[name] = np.array(prediction_tr3)
      test_actuals3[name] = np.array(y_test_temp_tr)
      
      # cm[name] = cm
      predictions_tr3[name] = prediction_tr3
      predictions_train_tr3[name] = prediction_train_tr3
      print('************************************\n\n')

for dataset_key in datasets.keys():
  for key in mmodels.keys():
      print("Nama dataset : ", dataset_key)
      print("Nama model : ", key)
      name = "Tiga Cluster " + key + " " + dataset_key + " dengan Imbalance Treatement"

      # define pipeline
      # transform the dataset

      strategy = {1:round(len(datasets[dataset_key]) * 0.25), 2:round(len(datasets[dataset_key]) * 0.25)}
      over = SMOTE(sampling_strategy=strategy)
      steps = [('o', over)]
      pipeline = Pipeline(steps=steps)
      # transform the dataset
      X, y = pipeline.fit_resample(datasets[dataset_key].iloc[:,:-2], datasets[dataset_key].iloc[:,-2:-1])
      unique, counts = np.unique(y, return_counts=True)
      print("Distribusi Tiga Cluster",dict(zip(unique, counts)),"\n")

      X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

      # Fit the classifier model
      with warnings.catch_warnings():
          # ignore all caught warnings
          warnings.filterwarnings("ignore")
          # execute code that will generate warnings
          mmodels[key].fit(X_train_temp_tr, y_train_temp_tr)
      
      # Prediction 
      prediction_tr3 = mmodels[key].predict(X_test_temp_tr)
      prediction_train_tr3 = mmodels[key].predict(X_train_temp_tr)
      
      
      # Calculate Accuracy, Precision and Recall Metrics
      accuracy3[name] = accuracy_score(prediction_tr3, y_test_temp_tr)
      precision3[name] = precision_score(prediction_tr3, y_test_temp_tr, average=None)
      recall3[name] = recall_score(prediction_tr3, y_test_temp_tr, average=None)
      score_tr3[name] = mmodels[key].score(X_train_temp_tr, y_train_temp_tr)

      train_predicts3[name] = np.array(prediction_train_tr3)
      train_actuals3[name] = np.array(y_train_temp_tr)
      test_predicts3[name] = np.array(prediction_tr3)
      test_actuals3[name] = np.array(y_test_temp_tr)
      
      # cm[name] = cm
      predictions_tr3[name] = prediction_tr3
      predictions_train_tr3[name] = prediction_train_tr3
      print('************************************\n\n')

# pemodelan deep learning
from keras.utils import np_utils
for dataset_key in datasets.keys():
    print("Nama dataset : ", dataset_key)
    print("Nama model : ", "Deep Learning")
    name = "Tiga Cluster " + "Deep Learning" + " " + dataset_key + " tanpa Imbalance Treatement"

    # tanpa transformasi
    X, y = datasets[dataset_key].iloc[:,:-2], np_utils.to_categorical(datasets[dataset_key].iloc[:,-2:-1])
    unique, counts = np.unique(y, return_counts=True)
    print("Distribusi Tiga Cluster",dict(zip(unique, counts)),"\n")

    X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

    # Fit the classifier model
    with warnings.catch_warnings():
        # ignore all caught warnings
        warnings.filterwarnings("ignore")
        # execute code that will generate warnings
        backend.clear_session()
        np.random.seed(42)
        random.seed(42)

        dl_tiga = Sequential()
        dl_tiga.add(Dense(32, activation='relu', input_dim=X_train_temp_tr.shape[1]))
        dl_tiga.add(Dense(3, activation='softmax'))
        # optimizer = tf.keras.optimizers.Adam(0.001)
        dl_tiga.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'] )
        
    history = dl_tiga.fit(X_train_temp_tr, y_train_temp_tr, validation_split=0.2, epochs=100, batch_size=32, verbose=1)
    
    # Prediction 
    prediction_tr3 = dl_tiga.predict(X_test_temp_tr)
    prediction_train_tr3 = dl_tiga.predict(X_train_temp_tr)
    
    # Calculate Accuracy, Precision and Recall Metrics
    accuracy3[name] = accuracy_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1))
    precision3[name] = precision_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1), average=None)
    recall3[name] = recall_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1), average=None)
    score_tr3[name] = accuracy_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1))

    train_predicts3[name] = np.array(prediction_train_tr3.argmax(axis=1))
    train_actuals3[name] = np.array(y_train_temp_tr.argmax(axis=1))
    test_predicts3[name] = np.array(prediction_tr3.argmax(axis=1))
    test_actuals3[name] = np.array(y_test_temp_tr.argmax(axis=1))
    
    # cm[name] = cm
    predictions_tr3[name] = prediction_tr3.argmax(axis=1)
    predictions_train_tr3[name] = prediction_train_tr3.argmax(axis=1)
    print('************************************\n\n')

for dataset_key in datasets.keys():
    print("Nama dataset : ", dataset_key)
    print("Nama model : ", "Deep Learning")
    name = "Tiga Cluster " + "Deep Learning" + " " + dataset_key + " dengan Imbalance Treatement"

    # define pipeline
    strategy = {1:round(len(datasets[dataset_key]) * 0.25), 2:round(len(datasets[dataset_key]) * 0.25)}
    over = SMOTE(sampling_strategy=strategy)
    steps = [('o', over)]
    pipeline = Pipeline(steps=steps)
    # transform the dataset
    X, y = pipeline.fit_resample(datasets[dataset_key].iloc[:,:-2], np_utils.to_categorical(datasets[dataset_key].iloc[:,-2:-1])) 
    unique, counts = np.unique(y, return_counts=True)
    print("Distribusi Tiga Cluster",dict(zip(unique, counts)),"\n")

    X_train_temp_tr, X_test_temp_tr, y_train_temp_tr, y_test_temp_tr = train_test_split(X, y, test_size=0.25, random_state=42)

    # Fit the classifier model
    with warnings.catch_warnings():
      # ignore all caught warnings
      warnings.filterwarnings("ignore")
      # execute code that will generate warnings
      backend.clear_session()
      np.random.seed(42)
      random.seed(42)

      dl_tiga = Sequential()
      dl_tiga.add(Dense(32, activation='relu', input_dim=X_train_temp_tr.shape[1]))
      dl_tiga.add(Dense(3, activation='softmax'))
      # optimizer = tf.keras.optimizers.Adam(0.001)
      dl_tiga.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'] )
      
    history3 = dl_tiga.fit(X_train_temp_tr, y_train_temp_tr, validation_split=0.2, epochs=100, batch_size=32, verbose=1)
    
    # Prediction 
    prediction_tr3 = dl_tiga.predict(X_test_temp_tr)
    prediction_train_tr3 = dl_tiga.predict(X_train_temp_tr)
    
    # if len(set(prediction_tr)) != 2:
    #     pre, rec, thre = precision_recall_curve(y_test_temp_tr, prediction_tr)
    #     print('Thresold', thre[np.argmax(pre*rec)])
    #     for i in range(0, len(prediction_tr)):
    #         if(prediction_tr[i] >= thre[np.argmax(pre*rec)]):
    #             prediction_tr[i] = 1
    #         else:
    #             prediction_tr[i] = 0
    
    # Calculate Accuracy, Precision and Recall Metrics
    accuracy3[name] = accuracy_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1))
    precision3[name] = precision_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1), average=None)
    recall3[name] = recall_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1), average=None)
    score_tr3[name] = accuracy_score(prediction_tr3.argmax(axis=1), y_test_temp_tr.argmax(axis=1))

    train_predicts3[name] = np.array(prediction_train_tr3.argmax(axis=1))
    train_actuals3[name] = np.array(y_train_temp_tr.argmax(axis=1))
    test_predicts3[name] = np.array(prediction_tr3.argmax(axis=1))
    test_actuals3[name] = np.array(y_test_temp_tr.argmax(axis=1))
    
    # cm[name] = cm
    predictions_tr3[name] = prediction_tr3.argmax(axis=1)
    predictions_train_tr3[name] = prediction_train_tr3.argmax(axis=1)
    print('************************************\n\n')

metode_arr3 = []
better_models3 = []

for key in score_tr3.keys():
    print(key)
    
    train_predict_tr3 = train_predicts3[key]
    train_actual_tr3 = train_actuals3[key]
    test_predict_tr3 = test_predicts3[key]
    test_actual_tr3 = test_actuals3[key]
    
    matrix_tr3 = classification_report(test_actual_tr3, test_predict_tr3)
    accuracy_temp3 = accuracy_score(test_predict_tr3, test_actual_tr3)
    precision_temp3 = precision_score(test_predict_tr3, test_actual_tr3, average=None)
    recall_temp3 = recall_score(test_predict_tr3, test_actual_tr3, average=None)

    print('Accuracy : ', accuracy_temp3)
    print('Presisi : ', precision_temp3)
    print('Recall : ', recall_temp3)
    print('Classification report : \n',matrix_tr3)

    metode_arr3.append({'Nama Pemodelan' : key, 'Akurasi' : accuracy_temp3, 'Presisi 0' : precision_temp3[0], 'Presisi 1' : precision_temp3[1], 'Presisi 2' : precision_temp3[2], 'Recall 0' : recall_temp3[0], 'Recall 1' : recall_temp3[1], 'Recall 2' : recall_temp3[2]})

metode_df3 = pd.DataFrame.from_dict(metode_arr3)

# Data tanpa penangan Imbalance
metode_df3[metode_df3['Nama Pemodelan'].str.contains("tanpa")].sort_values(by=['Akurasi', 'Presisi 0', 'Presisi 1', 'Presisi 2', 'Recall 0', 'Recall 1', 'Recall 2'], ascending=False)

# Data dengan penangan Imbalance
metode_df3[metode_df3['Nama Pemodelan'].str.contains("dengan")].sort_values(by=['Akurasi', 'Presisi 0', 'Presisi 1', 'Presisi 2', 'Recall 0', 'Recall 1', 'Recall 2'], ascending=False)

"""Dari beberapa hasil diatas, didapatkan beberapa kesimpulan terkait pemodelan dengan **tiga kluster** antara lain:

*   Pemodelan dengan akurasi, presisi, dan recall yang paling baik dalam model yang diajukan sebagian besar menggunakan metode XGBBoost
*   Data penanganan *imbalance data* menggunakan oversampling dengan menjadikan jumlah label minor sebesar 30% dari label mayor mampu meningkatkan presisi minimal 60% disalah satu kelompok dengan *imbalance treatment*
* Reduksi yang dilakukan dengan menggunakan metode linier (PCA dan SVD) dapat menunjukkan lebih baik dan dimensi kecil dengan *cumulative variance* 70% sudah dapat hampir mengimbangi *cumulative variance* 95%.
* Untuk tiga cluster, model lebih baik dengan penanganan data imbalance. Selain itu, model tiga cluster lebih representatif dibandingkan dua cluster

### Kesimpulan

1.   Penggunaan belanja barang pemerintah daerah memiliki subrincian yang kompleks serta didalamnya masih dibagi menjadi beberapa fungsi pemerintahan, sehingga dalam kondisi data dipisahkan oleh wilayah menunjukkan matriks yang besar terhadap kolom daripada baris.
2.   Reduksi dimensi merupakan salah satu cara mengatasi jumlah kolom yang besar. Beberapa metode percobaan klasifikasi dengan beberapa metode dimensi reduksi menunjukkan pemodelan dengan XGBBoost dan PCA dengan cumulative variance 95% menunjukkan penilaian model yang lebih baik pada bagian akurasi, presisi, dan recall.
3. Pemodelan dengan tiga cluster lebih baik menggambarkan kondisi kelompok pembangunan secara grafik dibandingkan dengan dua kluster (*binary*).
4. Penanganan *imbalance data* hasil klasifikasi juga menunjukkan model yang lebih dalam peningkatan akurasi, presisi, dan recall.
"""

